{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn import linear_model\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from __future__ import print_function\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "import itertools\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Propensity score matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "#### Dataset description\n",
    "\n",
    "- `treat`: 1 if the subject participated in the job training program, 0 otherwise\n",
    "- `age`: the subject's age\n",
    "- `educ`: years of education\n",
    "- `race`: categorical variable with three possible values: Black, Hispanic, or White\n",
    "- `married`: 1 if the subject was married at the time of the training program, 0 otherwise\n",
    "- `nodegree`: 1 if the subject has earned no school degree, 0 otherwise\n",
    "- `re74`: real earnings in 1974 (pre-treatment)\n",
    "- `re75`: real earnings in 1975 (pre-treatment)\n",
    "- `re78`: real earnings in 1978 (outcome)\n",
    "\n",
    "The dataset contains 614 observations.\n",
    "We observe that the categorical variables are encoded using dummy varibales, i.e. binary variables. We are going to stick with this encoding as it is advantegous for the regression task that we are going to perform later. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalonde_data = pd.read_csv(\"lalonde.csv\")\n",
    "lalonde_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lalonde_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A naive analysis\n",
    "\n",
    "To begin with, we compare the distribution of the outcome variable (re78) between the two groups in a naive way. This means that we do not consider relationships between the outcome variable and other available features, and just focus on the differences (possibly) induced by participating in the job training program.\n",
    "\n",
    "We compare the distributions based on:\n",
    "\n",
    "1. Boxplot diagrams\n",
    "2. Histograms\n",
    "3. QQ plots\n",
    "4. Summary statistics\n",
    "5. Mann-Whitney-U test (sometimes also referred to as Wilcoxon signed rank test.)\n",
    "\n",
    "For these steps of the analysis we define the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treat = {0: 'treated', 1: 'control'}\n",
    "\n",
    "def draw_box(df, col):\n",
    "    \"\"\"Draw a box plot for the values of the specified column for each of the two groups\"\"\"\n",
    "    df.boxplot(by='treat', column=col, figsize = [5, 5], grid=False)\n",
    "    plt.show()\n",
    "\n",
    "def draw_hist(df, col):\n",
    "    \"\"\"Draw histograms and kernel density estimation plots for the specified column.\n",
    "    Two plots are created for the two groups but displayed in two overlapping layers for comparison.\n",
    "    \"\"\"\n",
    "    fig_hist, axs_hist = plt.subplots(nrows=2, sharex=True)\n",
    "    df.groupby(\"treat\")[col].plot(kind='kde', ax=axs_hist[1])\n",
    "    axs_hist[1].set_xlabel(col)\n",
    "    axs_hist[1].set_xlim(xmin=0)\n",
    "    groups = df.groupby(\"treat\")[col]\n",
    "    for k, v in groups:\n",
    "        v.hist(label=treat[k], alpha=.75, ax=axs_hist[0], bins=15, range=[df[col].min(), df[col].max()], figsize = [8, 5])\n",
    "    axs_hist[0].legend()\n",
    "    axs_hist[0].set_ylabel('number of participants')\n",
    "    plt.show()\n",
    "\n",
    "def draw_qqs(df, col):\n",
    "    \"\"\"Draw a QQ plot for both groups to find out more about the distribution of the data.\n",
    "    NB: A comparison to a normal distribution with fitted parameters is performed.\"\"\"\n",
    "    fig_qq, axs_qq = plt.subplots(ncols=2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "    stats.probplot(df[df.treat == 0][col], dist=\"norm\", plot=axs_qq[0])\n",
    "    axs_qq[0].set_title('Control Group')\n",
    "    axs_qq[0].set_ylim(ymin=0)\n",
    "    stats.probplot(df[df.treat == 1][col], dist=\"norm\", plot=axs_qq[1])\n",
    "    axs_qq[1].set_title('Treatment Group')\n",
    "    plt.show()\n",
    "    \n",
    "def get_summary(df, col):\n",
    "    \"\"\"Print summary statistics for both groups.\"\"\"\n",
    "    print('Control group')\n",
    "    print('================================================')\n",
    "    print(round(df[df.treat == 0][col].describe(), 2))\n",
    "    print('================================================')\n",
    "    print()\n",
    "    print('Treatment group')\n",
    "    print('================================================')\n",
    "    print(round(df[df.treat == 1][col].describe(), 2))\n",
    "\n",
    "def test(df, col):\n",
    "    \"\"\"Perform Mann-Whitney-U test and pretty print result.\"\"\" \n",
    "    s, p = stats.mannwhitneyu(df[df.treat == 0][col], df[df.treat == 1][col])\n",
    "    print('Mann-Whitney-U test:')\n",
    "    print('================================================')\n",
    "    print('p-value: ', round(p, 4))\n",
    "    print('test statistic: ', round(s, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we define a function that invokes all the functions for numerical features and displays the output in a structured way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_numeric(df, col, detailed=True):    \n",
    "    \"\"\"Perform analysis for a numerical feature and pretty print result.\"\"\"\n",
    "    print('=================================================================================')\n",
    "    print('                                  ', col)\n",
    "    print('=================================================================================')\n",
    "    draw_box(df, col)\n",
    "    draw_hist(df, col)\n",
    "    if(detailed):\n",
    "        draw_qqs(df, col)\n",
    "    print()\n",
    "    get_summary(df, col)\n",
    "    print()\n",
    "    test(df, col)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now analyse the distribution of the real earnings in 1978 within the two groups. The boxplot diagram shows that the median earning differs only slightly between the groups, and also the distributions are similar considering the interquartile range which is a measure of dispersion. However, we observe that there are some outliers in the treatment group that exhibit high earnings. The tables below give a numerical summary of the distribution of earnings within the groups. Interestingly, the upper quartile earning is higher in the control than in the treatment group.\n",
    "The similarity between the two distributions is even more clearly visible from the kernel density estimates (or the histograms). To determine whether the observed (minor) differences are of significance, statistical test shall be computed. With numeric features, the t-test is a common means to test differences between the mean value of two groups. One necessary condition to use the t-test is that the features follow a normal distribution. To determine this, Quantile-Quantile Plots are drawn which show clear deviations from normality (normality would be fulfilled if the blue points would be evenly grouped around the diagonal). In the absence of normality, the Wilcoxon signed rank test is a nonparametric alternative. Here, we compare group medians instead of means. Regarding the earnings in 1978, the Wilcoxon test shows no significant difference between the medians of the two groups at a test level of 0.05 (p-value = 0.1409). This is in accordance with our observations from the boxplot diagram or histogram/density estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data, 're78')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A closer look at the data\n",
    "\n",
    "In order to get a better understanding of whether our naive analysis was appropriate, we now compare the distributions of the other covariates within the treatment groups. This is necessary as the observed sample was not randomly drawn. If differences are observed, these could influence the outcome of the above analysis as one would assume, for example, older people to have higher income in general. The effect of the treatment might then be confounded with that of the age effect. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of quantitative features\n",
    "\n",
    "To analyse possible imbalance, we analyse the numerical features using the same strategy as in Part 1.\n",
    "\n",
    "Considering the age distribution between treatment and control group, differences are apparent. People in the treatment group tend to be younger than in the control group, although the median age is not significantly different (Wilcoxon rank sum test, p-value = 0.2598).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data, 'age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering years of education, differences are far less apparent. True, the control group exhibits outliers in both directions, but the median number of education years as well as the interquartile range is similar in both groups. Wilcoxon rank sum test gives again no significant result (p-value = 0.396)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data, 'educ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the earnings in 1974 show significant differences between the two groups. The boxplot diagram (or consult the tables below) shows that the median earning in the treatment group is 0, and people earning more than about 4000 are considered outliers. This difference is supported by a significant test result (p-value < 0.001), indicating that the earnings in 1974 were indeed significantly different between the two groups. The kernel density estimation is again a different way of visualizing this difference, showing that the mean earning in the control group is indeed higher than the treatment's group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data, 're74')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In concordance with the earnings in 1974, the same conclusions can be drawn for the earning in the consecutive year 1975. Here again, earnings are higher in the control than in the treatment group. These obvious differences lead us to conclude that there is a certain selection bias as to which persons are in the treatment group. This makes sense as people with less income or unemployed people might be more open to participating in a job training program. Again, Mann Whitney U Test gives a significant result (p-value < 0.001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data, 're75')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of categorical features\n",
    "\n",
    "It remains to take a closer look at the categorical covariates. As the methods used above are not appropriate for categorical variables, we need to define new functions for the analysis. For example, there is no point in drawing histograms for categrical variables, the thing to use here are bar plots. The chi-square test is used to analyse possible group differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bar(df, col):\n",
    "    '''Draw a bar plot of the number of values in each category for the two groups'''\n",
    "    df_grouped = df.groupby(['treat', col])[col].count()\n",
    "    df_grouped = df_grouped.unstack()\n",
    "    display(df_grouped)\n",
    "    df_grouped = df_grouped.div(df_grouped.sum(axis=1), axis=0)\n",
    "    pl = df_grouped.plot(kind='bar', figsize=[5,5], rot=0)\n",
    "    pl.set_title(col)\n",
    "    pl.set_ylabel('proportion of participants')\n",
    "    pl.set_xlabel('treatment group')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the race feature some preprocessing is needed in order to be able to draw a meaningful bar plot. Especially, the data frame does not contain a column for white people. We assume that individuals are white in case they are neither black nor hispanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_race(df):\n",
    "    '''Draw a bar plot for the race feature.'''\n",
    "    df['white'] = (~(df['black'].astype(bool) | df['hispan'].astype(bool))).astype(int)\n",
    "    df_grouped = df.groupby(df.treat)['white', 'black', 'hispan'].sum()\n",
    "    del df['white']\n",
    "    display(df_grouped)\n",
    "    df_grouped = df_grouped.div(df_grouped.sum(axis=1), axis=0)\n",
    "    pl = df_grouped.plot(kind='bar', figsize=[7,5], rot=0)\n",
    "    pl.set_title('race')\n",
    "    pl.set_ylabel('proportion of participants')\n",
    "    pl.set_xlabel('treatment group')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_chisq(df, table):\n",
    "    '''Perform chi square test'''\n",
    "    print('Chi-square test')\n",
    "    print('================================================')\n",
    "    #perform test\n",
    "    s, p, _, _ = stats.chi2_contingency(table)\n",
    "    print('p-value: ', round(p, 4))\n",
    "    print('test statistic: ', round(s, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_categoric(df, col):    \n",
    "    \"\"\"Perform analysis for a categorical feature and pretty print result.\"\"\"\n",
    "    print('=================================================================================')\n",
    "    print('                                  ', col)\n",
    "    print('=================================================================================')\n",
    "    draw_bar(df, col)\n",
    "    print()\n",
    "    \n",
    "    #compute contingency table and perform test\n",
    "    df['neg'] = df[col].apply(lambda x: 1-x)\n",
    "    table = df.groupby(df.treat)[col, 'neg'].sum()\n",
    "    del df['neg']\n",
    "    test_chisq(df, table)\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_race(df):    \n",
    "    \"\"\"Perform analysis for the race feature and pretty print result.\"\"\"\n",
    "    print('=================================================================================')\n",
    "    print('                                  race')\n",
    "    print('=================================================================================')\n",
    "    plot_race(df)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    #compute contingency table and perform test\n",
    "    df['white'] = (~(df['black'].astype(bool) | df['hispan'].astype(bool))).astype(int)\n",
    "    table = df.groupby(df.treat)['white', 'black', 'hispan'].sum()\n",
    "    del df['white']\n",
    "    \n",
    "    test_chisq(df, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the treatment group we find relatively more married people than in the control group. This difference is significant, or to put it in other words: there is a significant dependence between the marital state and group membership (Chi-square test, p-value < 0.001). However, this may also be due to the fact that the treatment group holds more young people, which tend to be not (yet) married. Here again might be some kind of confounding.analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_categoric(lalonde_data, 'married')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same applies to *nodegree*, where we could find a significant relationship between group membership and *nodegree* (p-value = 0.0113)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_categoric(lalonde_data, 'nodegree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the participants race is included in the data as is often so with US studies. Interestingly, mostly black people participate in the treatment group. There are various studies analyzing the relationship between race and, for example, earnings. So here again, we find a possible source for confounding. The difference in earnings in the years 1974/5 might well be due to this difference in race structure between the two groups. However, no further analysis is conducted in this direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_race(lalonde_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A propensity score model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate propensity scores using logistic regression, that is we estimate the probability of receiving the treatment, based on the pre-treatment features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "#input features: everything but id, treat, and re78\n",
    "logistic = logistic.fit(lalonde_data.drop(lalonde_data.columns[[0, 1, -1]], axis=1), lalonde_data.treat)\n",
    "propensity_scores = logistic.predict_proba(lalonde_data.drop(lalonde_data.columns[[0, 1, -1]], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column of the result contains the relevant score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logistic.classes_)\n",
    "print(propensity_scores[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the result in a new column of the data frame in order to be able to use it for matching in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalonde_data['propensity'] = propensity_scores[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Balancing the dataset via matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group. Furthermore, we want to minimize the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects.\n",
    "\n",
    "As explained in the lecture this matching problem corresponds to finding a matching in a bipartite graph where the two groups correspond two the two sets of nodes. We therefore construct the corresponding graph from our dataset. For the python represetation of the graph we use the `networkx` package. We can then use the `max_weight_matching` function provided by the package to compute the matching.\n",
    "\n",
    "To construct the graph we first compute the crossproduct of the observations in the treated group and the observations in the control group. That is, each data point from the treated group is combined with every data point from the control group. Then we compute a weight for each combination. Note that the goal is to minimize the sum of absolute propensity-score differences, while the package only provides a maximization method. So instead of minimizing the sum of absolute differences, we maximize the sum of negated absolute differences. However, the algorithm can not deal with negative weights. Therefore, we add 1 to every weight. This ensures that the weights are positive (as the difference of propensity scores can be maximal 1), while it does not affect the relative order of the weights. Therefore, the same edges will be chosen in the end.\n",
    "\n",
    "For the selected nodes the function returns a mapping, where each node is mapped to its partner. Therfore we can simply use the key list of this mapping to restrict the data set and for the balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = pd.merge(lalonde_data.reset_index()[lalonde_data.treat == 1].assign(key=0), lalonde_data.reset_index()[lalonde_data.treat == 0].assign(key=0), on='key').drop('key', axis=1)\n",
    "graph_data['weight'] = - np.abs(graph_data['propensity_x'] - graph_data['propensity_y']) + 1\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(zip(graph_data.index_x, graph_data.index_y, graph_data.weight))\n",
    "matching = nx.max_weight_matching(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convince ourselves that indeed all data points of the treated group are contained in the matching and that that each data point from the control group is matched with at most one data point from the treated group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matching) == 2 * len(lalonde_data[lalonde_data.treat == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in matching.items():\n",
    "    if(matching[value]!=key):\n",
    "        print('something went wrong!')\n",
    "print('Everything alright!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(matching.values()) == len(set(matching.values()))):\n",
    "    print('Everything alright!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalonde_data_balanced = lalonde_data.iloc[list(matching.keys())]\n",
    "lalonde_data_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis as in part 1 and 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After balancing in the previous step, the comparison of earnings in the treatment and control group shows again no significant difference in earnings (p-value = 0.1754). However, the earnings' distributions have changed such that earnings in the treatment tend now to be slightly higher than those in the control group (which was just the other way round before balancing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data_balanced, 're78', detailed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other available features' distributions changed for some features. The distributions of age, as well as earnings in 1974 are still significantly different (p-value < 0.0001; p-value = 0.0002), although the differences are now much smaller regarding earnings in 1974. The distribution of years of education showed no significant difference before matching, neither does it afterwards. The same goes for earnings in 1975 (p-value = 0.0034). Martial state as well as `nodegree` show balanced proportions after matching, while the race membership still shows significant differences between the groups (p-value < 0.0001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data_balanced, 'age', detailed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data_balanced, 'educ', detailed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data_balanced, 're74', detailed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data_balanced, 're75', detailed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_categoric(lalonde_data_balanced, 'married')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_categoric(lalonde_data_balanced, 'nodegree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_race(lalonde_data_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These obvious differences in distributions indicate that the balancing progress did not address all features properly. Especially the imbalance in race is a serious problem as we might assume a strong relationship between race and earnings in general, at least back in 1978. Therefore it is essential that the sample be balanced concerning race at least. Also, the balancing of the age feature is still problematic. This could affect the outcome, as it is likely that there is a relationship between age and earnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Balancing the groups further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, one of the most problematic features is race. Therefore, we try to improve the balancing by making sure that only subjects that have the same race can be matched. The reason that we chose race over other problematic features like age for the refinement is that matching on equality makes more sense for categoric features with few possible values.\n",
    "\n",
    "\n",
    "We use the same matching procedure as above, but construct a slightly different graph. In particular, instead of computing the crossproduct of data points in the two groups, we join the two groups based on the `black` and `hispan` columns, i.e., on the race. Hence, in the constructed graph there are no edges between nodes corresponding to subjects with different races and they therefore cannot be selected in the matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = pd.merge(lalonde_data.reset_index()[lalonde_data.treat == 1], lalonde_data.reset_index()[lalonde_data.treat == 0], on=['black', 'hispan'])\n",
    "graph_data['weight'] = 1 - np.abs(graph_data['propensity_x'] - graph_data['propensity_y'])\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(zip(graph_data.index_x, graph_data.index_y, graph_data.weight))\n",
    "matching = nx.max_weight_matching(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalonde_data_balanced = lalonde_data.iloc[list(matching.keys())]\n",
    "lalonde_data_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lalonde_data_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After balancing for race, proportions are now equal between the two groups. The age distribution still exhibits differences (p-value = 0.0088), the control group consisting of slightly older people. Also, the other features do not exhibit interesting changes compared to the first balancing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data_balanced, 'age', detailed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data_balanced, 'educ', detailed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data_balanced, 're74', detailed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data_balanced, 're75', detailed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_categoric(lalonde_data_balanced, 'married')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_categoric(lalonde_data_balanced, 'nodegree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_race(lalonde_data_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. A less naive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After two-step balancing, we again consider the distribution of the outcome variable (earnings in 1978). Both the boxplot diagram and the density estimates indicate a difference in earnings between treatment and control group. While the median earning in the control group was 2283 USD, it was higher in the treatment group at 5131 USD. This difference in medians is statistically significant (p-value = 0.0249). From this we conclude that the job training program was indeed effective, quite contrary to the impression we got in the initial analysis (before matching). This shows the importance of considering the data collection process and data quality. In the absence of a sampling design featuring random sampling, balancing techniques can be applied to draw valid conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_numeric(lalonde_data_balanced, 're78', detailed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Applied ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing sklearn modules\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data\n",
    "___\n",
    "This function preprocess our dataset:\n",
    "- TF-IDF, is used to represent the importance of the words inside their context\n",
    "- SPLIT, is used to split the dataset into two portions defined by RATIO of **training** and **testing**\n",
    "\n",
    "The stratify parameter is used to balance the distribution of training and testing set over all the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, vectorizer, ratio=0.9):\n",
    "    '''\n",
    "    Function used to preprocess the dataset, vectorize, and split it in training and testing portions.\n",
    "    Params:\n",
    "        @data: the dataset that will be split and vectorized\n",
    "        @vectorizer: used to vectorize the data\n",
    "        @ratio: the ratio used to split the dataset\n",
    "    '''\n",
    "    X = vectorizer.fit_transform(data.data)\n",
    "    return train_test_split(X, data.target, test_size=1-ratio, random_state=0, stratify=data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarse-to-fine approach for hyper parameters optimization\n",
    "To find the best parameters is commonly used a coarse-to-fine approach.\n",
    "We first start with a wide range of parameters and we refine them as we get closer to the best results.\n",
    "___\n",
    "We declare the parameters we want to optimize:\n",
    "- More parameters =  better exploring power \n",
    "- Less parameters = decreasing processing time\n",
    "\n",
    "In the GridSearchCV function is possible to choose several parameters, in particular:\n",
    "- **n_jobs**, are the threads used in the computations\n",
    "- **verbose**, will show in verbose mode the output of the training\n",
    "- **cv**, it's possible to specify how many folds to use in the cross validation\n",
    "- **scoring**, the metric used for the training.\n",
    "\n",
    "While the first parameters will not impact so much in the effective accuracy of the classifier, it's important to define a good scoring metric.\n",
    "\n",
    "In our classification problem we need to maximize the number of samples that we correctly predict. For this reason we need to have a scoring metrics that focuses on the number of correct samples predicted over the total.\n",
    "After this reasoning we choose to select the *'accuracy'* metric, which is the scoring metric the does what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_parameters(train_X, train_Y, pipeline, parameters, n_jobs=4, verbose=1, cv=5):\n",
    "    '''\n",
    "    Function used to optimize parameters of the RandomForest using GridSearchCV.\n",
    "    Params:\n",
    "        @train_X: features dataset of training\n",
    "        @train_Y: label dataset of training\n",
    "        @pipeline: the parameters that are optimized \n",
    "        @parameters: the parameters ranges used for the optimization\n",
    "        @n_jobs, used to parallelize the training.\n",
    "        @verbose, output training in verbose mode.\n",
    "        @cv, k-fold used.\n",
    "    '''\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=n_jobs, verbose=verbose, cv=cv, scoring='accuracy')\n",
    "    print('----------------------------------------')\n",
    "    print(\"Cross validating with grid search for...\")\n",
    "    print([name for name, _ in pipeline.steps])\n",
    "    print('----------------------------------------')\n",
    "    print(\"Analyzed parameters:\")\n",
    "    pprint(parameters)\n",
    "    print('----------------------------------------')\n",
    "    t0 = time()\n",
    "    grid_search.fit(train_X, train_Y)\n",
    "    print(\"Fine tuning took %0.3fs\" % (time() - t0))\n",
    "    print(\"Best obtained is score: %0.3f\" % grid_search.best_score_)\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    # print(\"Grid score results\")\n",
    "    # print(grid_search.cv_results_) # USED TO SEE THE RESULTS OF EVERY PARAMETERS COMBINATION\n",
    "    return best_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Random Forest classifier\n",
    "___\n",
    "We build the Random Forest classifier specifying:\n",
    "- number of estimators\n",
    "- maximum depth of the forest\n",
    "\n",
    "We train the model and return the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train, n_estimators, max_depth):\n",
    "    '''\n",
    "    Function used to train the random forest classifier.\n",
    "    Params:\n",
    "        @train_X: features dataset of training\n",
    "        @train_Y: label dataset of training\n",
    "        @n_estimators: number of estimators in the random forest\n",
    "        @max_depth: max depth of the random forest\n",
    "    '''\n",
    "    rfc = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "    return rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting confusion matrix\n",
    "\n",
    "We plot the confusion matrix of our classifier given the classes names.\n",
    "__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cnf_matrix, class_names):\n",
    "    '''\n",
    "    Function used to plot the confusion matrix of the classified prediction.\n",
    "    Params:\n",
    "        @cnf_matrix: confusion matri of the prediction\n",
    "        @class_names: label names\n",
    "    '''\n",
    "    norm_conf = []\n",
    "    for i in cnf_matrix:\n",
    "        a = 0\n",
    "        tmp_arr = []\n",
    "        a = sum(i, 0)\n",
    "        for j in i:\n",
    "            tmp_arr.append(float(j)/float(a))\n",
    "        norm_conf.append(tmp_arr)\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    plt.clf()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect(1)\n",
    "    res = ax.imshow(np.array(norm_conf), cmap=plt.cm.jet, \n",
    "                    interpolation='nearest')\n",
    "    width, height = cnf_matrix.shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            ax.annotate(str(cnf_matrix[x][y]), xy=(y, x), \n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n",
    "    cb = fig.colorbar(res)\n",
    "    plt.xticks(range(width), class_names, rotation=90)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Prediction Label')\n",
    "    plt.yticks(range(height), class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize importance of features and the distribution of the importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features_importance(classifier, n_features=50, y='Feature importance'):\n",
    "    '''\n",
    "    Function used to plot the confusion matrix of the classified prediction.\n",
    "    Params:\n",
    "        @classifier: random forest classifier\n",
    "        @n_features: number of features to show\n",
    "        @y: label for the features importance - 'Feature Importance' by default\n",
    "    '''\n",
    "    importances_feat = pd.DataFrame(classifier.feature_importances_)\n",
    "    importances_feat.columns = [y]\n",
    "    importances_feat = importances_feat.sort_values(y,ascending=False)\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[16,10])\n",
    "    plt.suptitle('Distribution of feature importance', x=0.5, y=1.00, ha='center', fontsize='xx-large')\n",
    "    importances_feat.head(n_features).hist(ax=axes[0])\n",
    "    importances_feat.head(n_features).plot(ax=axes[1],kind='bar', title='Importance of the features')\n",
    "    return importances_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "___\n",
    "Fetching dataset and preprocessing using the functions described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "data = fetch_20newsgroups(subset='all')\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the dataset\n",
    "First thing we print the keys of the data, to check what are the columns that we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['target', 'filenames', 'data', 'description', 'target_names', 'DESCR'])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a frame to analyze its structure and then we print its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'document':data['data'], 'category':data['target']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cmu.edu&gt;\\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)\\nSubject: Which high-performance VLB video card?\\nSummary: Seek recommendations for VLB video card\\nNntp-Posting-Host: midway.ecn.uoknor.edu\\nOrganization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\\nKeywords: orchid, stealth, vlb\\nLines: 21\\n\\n  My brother is in the market for a high-performance video card that supports\\nVESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\\n\\n  - Diamond Stealth Pro Local Bus\\n\\n  - Orchid Farenheit 1280\\n\\n  - ATI Graphics Ultra Pro\\n\\n  - Any other high-performance VLB card\\n\\n\\nPlease post or email.  Thank you!\\n\\n  - Matt\\n\\n-- \\n    |  Matthew B. Lawson &lt;------------&gt; (mblawson@essex.ecn.uoknor.edu)  |   \\n  --+-- \"Now I, Nebuchadnezzar, praise and exalt and glorify the King  --+-- \\n    |   of heaven, because everything he does is right and all his ways  |   \\n    |   are just.\" - Nebuchadnezzar, king of Babylon, 562 B.C.           |   \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject: Re: ARMENIA SAYS IT COULD SHOOT DOWN TURKISH PLANES (Henrik)\\nLines: 95\\nNntp-Posting-Host: viktoria.dsv.su.se\\nReply-To: hilmi-er@dsv.su.se (Hilmi Eren)\\nOrganization: Dept. of Computer and Systems Sciences, Stockholm University\\n\\n\\n\\n\\n|&gt;The student of \"regional killings\" alias Davidian (not the Davidian religios sect) writes:\\n\\n\\n|&gt;Greater Armenia would stretch from Karabakh, to the Black Sea, to the\\n|&gt;Mediterranean, so if you use the term \"Greater Armenia\" use it with care.\\n\\n\\n\\tFinally you said what you dream about. Mediterranean???? That was new....\\n\\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\\n\\n\\n\\n\\n|&gt;It has always been up to the Azeris to end their announced winning of Karabakh \\n|&gt;by removing the Armenians! When the president of Azerbaijan, Elchibey, came to \\n|&gt;power last year, he announced he would be be \"swimming in Lake Sevan [in \\n|&gt;Armeniaxn] by July\".\\n\\t\\t*****\\n\\tIs't July in USA now????? Here in Sweden it's April and still cold.\\n\\tOr have you changed your calendar???\\n\\n\\n|&gt;Well, he was wrong! If Elchibey is going to shell the \\n|&gt;Armenians of Karabakh from Aghdam, his people will pay the price! If Elchibey \\n\\t\\t\\t\\t\\t\\t    ****************\\n|&gt;is going to shell Karabakh from Fizuli his people will pay the price! If \\n\\t\\t\\t\\t\\t\\t    ******************\\n|&gt;Elchibey thinks he can get away with bombing Armenia from the hills of \\n|&gt;Kelbajar, his people will pay the price. \\n\\t\\t\\t    ***************\\n\\n\\n\\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT's TRUE.\\n\\t\\n\\tSHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH\\n\\t\\t\\t\\t\\t\\t    **************\\n\\tBEING RAPED, KILLED AND TORTURED BY THE ARMENIANS??????????\\n\\t\\n\\tHAVE YOU HEARDED SOMETHING CALLED: \"GENEVA CONVENTION\"???????\\n\\tYOU FACIST!!!!!\\n\\n\\n\\n\\tOhhh i forgot, this is how Armenians fight, nobody has forgot\\n\\tyou killings, rapings and torture against the Kurds and Turks once\\n\\tupon a time!\\n      \\n       \\n\\n|&gt;And anyway, this \"60 \\n|&gt;Kurd refugee\" story, as have other stories, are simple fabrications sourced in \\n|&gt;Baku, modified in Ankara. Other examples of this are Armenia has no border \\n|&gt;with Iran, and the ridiculous story of the \"intercepting\" of Armenian military \\n|&gt;conversations as appeared in the New York Times supposedly translated by \\n|&gt;somebody unknown, from Armenian into Azeri Turkish, submitted by an unnamed \\n|&gt;\"special correspondent\" to the NY Times from Baku. Real accurate!\\n\\nOhhhh so swedish RedCross workers do lie they too? What ever you say\\n\"regional killer\", if you don't like the person then shoot him that's your policy.....l\\n\\n\\n|&gt;[HE]\\tSearch Turkish planes? You don't know what you are talking about.&lt;-------\\n|&gt;[HE]\\tsince it's content is announced to be weapons? \\t\\t\\t\\ti\\t \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n|&gt;Well, big mouth Ozal said military weapons are being provided to Azerbaijan\\ti\\n|&gt;from Turkey, yet Demirel and others say no. No wonder you are so confused!\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\tConfused?????\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\tYou facist when you delete text don't change it, i wrote:\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n        Search Turkish planes? You don't know what you are talking about.\\ti\\n        Turkey's government has announced that it's giving weapons  &lt;-----------i\\n        to Azerbadjan since Armenia started to attack Azerbadjan\\t\\t\\n        it self, not the Karabag province. So why search a plane for weapons\\t\\n        since it's content is announced to be weapons?   \\n\\n\\tIf there is one that's confused then that's you! We have the right (and we do)\\n\\tto give weapons to the Azeris, since Armenians started the fight in Azerbadjan!\\n \\n\\n|&gt;You are correct, all Turkish planes should be simply shot down! Nice, slow\\n|&gt;moving air transports!\\n\\n\\tShoot down with what? Armenian bread and butter? Or the arms and personel \\n\\tof the Russian army?\\n\\n\\n\\n\\nHilmi Eren\\nStockholm University\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubject: Re: IDE vs SCSI, DMA and detach\\nOriginator: guyd@pal500.austin.ibm.com\\nOrganization: IBM Austin\\nLines: 60\\n\\n\\nIn article &lt;1993Apr19.034517.12820@julian.uwo.ca&gt;, wlsmith@valve.heart.rri.uwo.ca (Wayne Smith) writes:\\n&gt; In article &lt;RICHK.93Apr15075248@gozer.grebyn.com&gt; richk@grebyn.com (Richard Krehbiel) writes:\\n&gt; &gt;&gt;     Can anyone explain in fairly simple terms why, if I get OS/2, I might \\n&gt; &gt;&gt;   need an SCSI controler rather than an IDE.  Will performance suffer that\\n&gt; &gt;&gt;   much?  For a 200MB or so drive?  If I don't have a tape drive or CD-ROM?\\n&gt; &gt;&gt;   Any help would be appreciated.\\n&gt; \\n&gt; &gt;So, when you've got multi-tasking, you want to increase performance by\\n&gt; &gt;increasing the amount of overlapping you do.\\n&gt; &gt;\\n&gt; &gt;One way is with DMA or bus mastering.  Either of these make it\\n&gt; &gt;possible for I/O devices to move their data into and out of memory\\n&gt; &gt;without interrupting the CPU.  The alternative is for the CPU to move\\n&gt; &gt;the data.  There are several SCSI interface cards that allow DMA and\\n&gt; &gt;bus mastering.\\n&gt;  ^^^^^^^^^^^^\\n&gt; How do you do bus-mastering on the ISA bus?\\n&gt; \\n&gt; &gt;IDE, however, is defined by the standard AT interface\\n&gt; &gt;created for the IBM PC AT, which requires the CPU to move all the data\\n&gt; &gt;bytes, with no DMA.\\n&gt; \\n&gt; If we're talking ISA (AT) bus here, then you can only have 1 DMA channel\\n&gt; active at any one time, presumably transferring data from a single device.\\n&gt; So even though you can have at least 7 devices on a SCSI bus, explain how\\n&gt; all 7 of those devices can to DMA transfers through a single SCSI card\\n&gt; to the ISA-AT bus at the same time.\\n\\nThink!\\n\\nIt's the SCSI card doing the DMA transfers NOT the disks...\\n\\nThe SCSI card can do DMA transfers containing data from any of the SCSI devices\\nit is attached when it wants to.\\n\\nAn important feature of SCSI is the ability to detach a device. This frees the\\nSCSI bus for other devices. This is typically used in a multi-tasking OS to\\nstart transfers on several devices. While each device is seeking the data the\\nbus is free for other commands and data transfers. When the devices are\\nready to transfer the data they can aquire the bus and send the data.\\n\\nOn an IDE bus when you start a transfer the bus is busy until the disk has seeked\\nthe data and transfered it. This is typically a 10-20ms second lock out for other\\nprocesses wanting the bus irrespective of transfer time.\\n\\n&gt; \\n&gt; Also, I'm still trying to track down a copy of IBM's AT reference book,\\n&gt; but from their PC technical manual (page 2-93):\\n&gt; \\n&gt; \"The (FDD) adapter is buffered on the I.O bus and uses the System Board\\n&gt; direct memory access (DMA) for record data transfers.\"\\n&gt; I expect to see something similar for the PC-AT HDD adapter.  \\n&gt; So the lowly low-density original PC FDD card used DMA and the PC-AT\\n&gt; HDD controller doesn't!?!?  That makes real sense.\\n-- \\n-- -----------------------------------------------------------------------------\\nGuy Dawson - Hoskyns Group Plc.\\n        guyd@hoskyns.co.uk  Tel Hoskyns UK     -  71 251 2128\\n        guyd@austin.ibm.com Tel IBM Austin USA - 512 838 3377\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew.cmu.edu&gt;\\nSubject: driver ??\\nOrganization: Sophomore, Mechanical Engineering, Carnegie Mellon, Pittsburgh, PA\\nLines: 15\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n \\n1)    I have an old Jasmine drive which I cannot use with my new system.\\n My understanding is that I have to upsate the driver with a more modern\\none in order to gain compatability with system 7.0.1.  does anyone know\\nof an inexpensive program to do this?  ( I have seen formatters for &lt;$20\\nbuit have no idea if they will work)\\n \\n2)     I have another ancient device, this one a tape drive for which\\nthe back utility freezes the system if I try to use it.  THe drive is a\\njasmine direct tape (bought used for $150 w/ 6 tapes, techmar\\nmechanism).  Essentially I have the same question as above, anyone know\\nof an inexpensive beckup utility I can use with system 7.0.1\\n \\nall help and advice appriciated.\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category  \\\n",
       "0  10         \n",
       "1  3          \n",
       "2  17         \n",
       "3  3          \n",
       "4  4          \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               document  \n",
       "0  From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1  From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)\\nSubject: Which high-performance VLB video card?\\nSummary: Seek recommendations for VLB video card\\nNntp-Posting-Host: midway.ecn.uoknor.edu\\nOrganization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\\nKeywords: orchid, stealth, vlb\\nLines: 21\\n\\n  My brother is in the market for a high-performance video card that supports\\nVESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\\n\\n  - Diamond Stealth Pro Local Bus\\n\\n  - Orchid Farenheit 1280\\n\\n  - ATI Graphics Ultra Pro\\n\\n  - Any other high-performance VLB card\\n\\n\\nPlease post or email.  Thank you!\\n\\n  - Matt\\n\\n-- \\n    |  Matthew B. Lawson <------------> (mblawson@essex.ecn.uoknor.edu)  |   \\n  --+-- \"Now I, Nebuchadnezzar, praise and exalt and glorify the King  --+-- \\n    |   of heaven, because everything he does is right and all his ways  |   \\n    |   are just.\" - Nebuchadnezzar, king of Babylon, 562 B.C.           |   \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject: Re: ARMENIA SAYS IT COULD SHOOT DOWN TURKISH PLANES (Henrik)\\nLines: 95\\nNntp-Posting-Host: viktoria.dsv.su.se\\nReply-To: hilmi-er@dsv.su.se (Hilmi Eren)\\nOrganization: Dept. of Computer and Systems Sciences, Stockholm University\\n\\n\\n\\n\\n|>The student of \"regional killings\" alias Davidian (not the Davidian religios sect) writes:\\n\\n\\n|>Greater Armenia would stretch from Karabakh, to the Black Sea, to the\\n|>Mediterranean, so if you use the term \"Greater Armenia\" use it with care.\\n\\n\\n\\tFinally you said what you dream about. Mediterranean???? That was new....\\n\\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\\n\\n\\n\\n\\n|>It has always been up to the Azeris to end their announced winning of Karabakh \\n|>by removing the Armenians! When the president of Azerbaijan, Elchibey, came to \\n|>power last year, he announced he would be be \"swimming in Lake Sevan [in \\n|>Armeniaxn] by July\".\\n\\t\\t*****\\n\\tIs't July in USA now????? Here in Sweden it's April and still cold.\\n\\tOr have you changed your calendar???\\n\\n\\n|>Well, he was wrong! If Elchibey is going to shell the \\n|>Armenians of Karabakh from Aghdam, his people will pay the price! If Elchibey \\n\\t\\t\\t\\t\\t\\t    ****************\\n|>is going to shell Karabakh from Fizuli his people will pay the price! If \\n\\t\\t\\t\\t\\t\\t    ******************\\n|>Elchibey thinks he can get away with bombing Armenia from the hills of \\n|>Kelbajar, his people will pay the price. \\n\\t\\t\\t    ***************\\n\\n\\n\\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT's TRUE.\\n\\t\\n\\tSHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH\\n\\t\\t\\t\\t\\t\\t    **************\\n\\tBEING RAPED, KILLED AND TORTURED BY THE ARMENIANS??????????\\n\\t\\n\\tHAVE YOU HEARDED SOMETHING CALLED: \"GENEVA CONVENTION\"???????\\n\\tYOU FACIST!!!!!\\n\\n\\n\\n\\tOhhh i forgot, this is how Armenians fight, nobody has forgot\\n\\tyou killings, rapings and torture against the Kurds and Turks once\\n\\tupon a time!\\n      \\n       \\n\\n|>And anyway, this \"60 \\n|>Kurd refugee\" story, as have other stories, are simple fabrications sourced in \\n|>Baku, modified in Ankara. Other examples of this are Armenia has no border \\n|>with Iran, and the ridiculous story of the \"intercepting\" of Armenian military \\n|>conversations as appeared in the New York Times supposedly translated by \\n|>somebody unknown, from Armenian into Azeri Turkish, submitted by an unnamed \\n|>\"special correspondent\" to the NY Times from Baku. Real accurate!\\n\\nOhhhh so swedish RedCross workers do lie they too? What ever you say\\n\"regional killer\", if you don't like the person then shoot him that's your policy.....l\\n\\n\\n|>[HE]\\tSearch Turkish planes? You don't know what you are talking about.<-------\\n|>[HE]\\tsince it's content is announced to be weapons? \\t\\t\\t\\ti\\t \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n|>Well, big mouth Ozal said military weapons are being provided to Azerbaijan\\ti\\n|>from Turkey, yet Demirel and others say no. No wonder you are so confused!\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\tConfused?????\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\tYou facist when you delete text don't change it, i wrote:\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n        Search Turkish planes? You don't know what you are talking about.\\ti\\n        Turkey's government has announced that it's giving weapons  <-----------i\\n        to Azerbadjan since Armenia started to attack Azerbadjan\\t\\t\\n        it self, not the Karabag province. So why search a plane for weapons\\t\\n        since it's content is announced to be weapons?   \\n\\n\\tIf there is one that's confused then that's you! We have the right (and we do)\\n\\tto give weapons to the Azeris, since Armenians started the fight in Azerbadjan!\\n \\n\\n|>You are correct, all Turkish planes should be simply shot down! Nice, slow\\n|>moving air transports!\\n\\n\\tShoot down with what? Armenian bread and butter? Or the arms and personel \\n\\tof the Russian army?\\n\\n\\n\\n\\nHilmi Eren\\nStockholm University\\n  \n",
       "3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubject: Re: IDE vs SCSI, DMA and detach\\nOriginator: guyd@pal500.austin.ibm.com\\nOrganization: IBM Austin\\nLines: 60\\n\\n\\nIn article <1993Apr19.034517.12820@julian.uwo.ca>, wlsmith@valve.heart.rri.uwo.ca (Wayne Smith) writes:\\n> In article <RICHK.93Apr15075248@gozer.grebyn.com> richk@grebyn.com (Richard Krehbiel) writes:\\n> >>     Can anyone explain in fairly simple terms why, if I get OS/2, I might \\n> >>   need an SCSI controler rather than an IDE.  Will performance suffer that\\n> >>   much?  For a 200MB or so drive?  If I don't have a tape drive or CD-ROM?\\n> >>   Any help would be appreciated.\\n> \\n> >So, when you've got multi-tasking, you want to increase performance by\\n> >increasing the amount of overlapping you do.\\n> >\\n> >One way is with DMA or bus mastering.  Either of these make it\\n> >possible for I/O devices to move their data into and out of memory\\n> >without interrupting the CPU.  The alternative is for the CPU to move\\n> >the data.  There are several SCSI interface cards that allow DMA and\\n> >bus mastering.\\n>  ^^^^^^^^^^^^\\n> How do you do bus-mastering on the ISA bus?\\n> \\n> >IDE, however, is defined by the standard AT interface\\n> >created for the IBM PC AT, which requires the CPU to move all the data\\n> >bytes, with no DMA.\\n> \\n> If we're talking ISA (AT) bus here, then you can only have 1 DMA channel\\n> active at any one time, presumably transferring data from a single device.\\n> So even though you can have at least 7 devices on a SCSI bus, explain how\\n> all 7 of those devices can to DMA transfers through a single SCSI card\\n> to the ISA-AT bus at the same time.\\n\\nThink!\\n\\nIt's the SCSI card doing the DMA transfers NOT the disks...\\n\\nThe SCSI card can do DMA transfers containing data from any of the SCSI devices\\nit is attached when it wants to.\\n\\nAn important feature of SCSI is the ability to detach a device. This frees the\\nSCSI bus for other devices. This is typically used in a multi-tasking OS to\\nstart transfers on several devices. While each device is seeking the data the\\nbus is free for other commands and data transfers. When the devices are\\nready to transfer the data they can aquire the bus and send the data.\\n\\nOn an IDE bus when you start a transfer the bus is busy until the disk has seeked\\nthe data and transfered it. This is typically a 10-20ms second lock out for other\\nprocesses wanting the bus irrespective of transfer time.\\n\\n> \\n> Also, I'm still trying to track down a copy of IBM's AT reference book,\\n> but from their PC technical manual (page 2-93):\\n> \\n> \"The (FDD) adapter is buffered on the I.O bus and uses the System Board\\n> direct memory access (DMA) for record data transfers.\"\\n> I expect to see something similar for the PC-AT HDD adapter.  \\n> So the lowly low-density original PC FDD card used DMA and the PC-AT\\n> HDD controller doesn't!?!?  That makes real sense.\\n-- \\n-- -----------------------------------------------------------------------------\\nGuy Dawson - Hoskyns Group Plc.\\n        guyd@hoskyns.co.uk  Tel Hoskyns UK     -  71 251 2128\\n        guyd@austin.ibm.com Tel IBM Austin USA - 512 838 3377\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "4  From: Alexander Samuel McDiarmid <am2o+@andrew.cmu.edu>\\nSubject: driver ??\\nOrganization: Sophomore, Mechanical Engineering, Carnegie Mellon, Pittsburgh, PA\\nLines: 15\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n \\n1)    I have an old Jasmine drive which I cannot use with my new system.\\n My understanding is that I have to upsate the driver with a more modern\\none in order to gain compatability with system 7.0.1.  does anyone know\\nof an inexpensive program to do this?  ( I have seen formatters for <$20\\nbuit have no idea if they will work)\\n \\n2)     I have another ancient device, this one a tape drive for which\\nthe back utility freezes the system if I try to use it.  THe drive is a\\njasmine direct tape (bought used for $150 w/ 6 tapes, techmar\\nmechanism).  Essentially I have the same question as above, anyone know\\nof an inexpensive beckup utility I can use with system 7.0.1\\n \\nall help and advice appriciated.\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset\n",
    "To train our Random Forest classifier we would like to split our dataset in a portion of training and testing. We are not splitting it in another portion for validation since the function **GridSearchCV** given by **Scikit-learn** takes care of the validation part (with the possibility to specify the number of folds _k_).\n",
    "\n",
    "We specify the ratio the we use to split the dataset and the vectorizer used to parse the documents (in our case a TF-IDF Vectorizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_data(data, vectorizer, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters declaration\n",
    "___\n",
    "- TfidfVectorizer is our TF-IDF vectorizer for the sentences\n",
    "- RandomForestClassifier is the classifier we are training\n",
    "\n",
    "Note:\n",
    "For further analysis it is possible to specify parameters also for the TfidfVectorizer, considering always that more parameters means more exploration power but more computation time also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pipeline\n",
    "pipeline = Pipeline([\n",
    "    #('vect', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters fine-tuning\n",
    "___\n",
    "Now we try to optimize our model by selecting a range for our parameters:\n",
    "- **max_depth**: between 20 and 40\n",
    "- **n_estimators**: between 300 and 500\n",
    "\n",
    "As we have seen so far, those are the most reasonable values for each parameters. We now run the GridSearchCV to optimize them and search for the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'clf__max_depth': (20, 30, 40),\n",
    "    'clf__n_estimators': (300, 400, 500),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Cross validating with grid search for...\n",
      "['clf']\n",
      "----------------------------------------\n",
      "Analyzed parameters:\n",
      "{'clf__max_depth': (20, 30, 40), 'clf__n_estimators': (300, 400, 500)}\n",
      "----------------------------------------\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed: 28.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning took 1868.124s\n",
      "Best obtained is score: 0.831\n"
     ]
    }
   ],
   "source": [
    "best_estimator = estimate_parameters(X_train, y_train, pipeline, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=40, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade-off - Performance vs Accuracy\n",
    "As we can see from the previous testing, the best results are obtained with the highest values of our parameters.\n",
    "So, we decide to increase the value of estimators and the max depth of the forest to see if we obtain better results without impacting to much on the performances.\n",
    "\n",
    "For the next testing:\n",
    "- **max_depth**: between 40 and 60\n",
    "- **n_estimators**: between 500 and 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'clf__max_depth': (40, 50, 60),\n",
    "    'clf__n_estimators': (500, 800, 1000),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Cross validating with grid search for...\n",
      "['clf']\n",
      "----------------------------------------\n",
      "Analyzed parameters:\n",
      "{'clf__max_depth': (40, 50, 60), 'clf__n_estimators': (500, 800, 1000)}\n",
      "----------------------------------------\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    }
   ],
   "source": [
    "best_estimator = estimate_parameters(X_train, y_train, pipeline, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on the parameters optimization\n",
    "Like the first time we obtain the best results with the highest values. This time the computation took a big amount of time. For this reason we will not explore other values since as we can notice we are already loosing in terms of performance without having a significant bonus in the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We train the RandomForest classifier with the best parameters obtained after our cross validation to predict the category of each news email. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = best_estimator.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = train_random_forest(X_train, y_train, \n",
    "                    params.get('clf__n_estimators'), \n",
    "                    params.get('clf__max_depth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "We test the classifier with the test set that we splitted before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_names = data.target_names\n",
    "plot_confusion_matrix(cnf_matrix, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "As we can see from the confusion matrix some of the categories are quite related, and this is why sometimes our classifier is not accurate. For instance **sci.electronics** is sometimes interpreted a **msc.forsale** which is intuitively right.\n",
    "The most evident category that we see is misunderstood is **talk.religion.misc**, which is often classified as **soc.religion.christian**, in fact both share common topics.\n",
    "\n",
    "We can state that our classifier performs well, however it still has some problems while trying to classify topic-related documents. In fact, the TF-IDF vectorizer assings importances to features based on their frequency in the document and their global frequency. This is the reason why our classifier encounters some problems when classifying similar categories.\n",
    "___\n",
    "### Plotting features importance and distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_feat = visualize_features_importance(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_feat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "As we can see from the values in the description above:\n",
    "- The entire first quartile is useless across all the decision trees.\n",
    "- The second quartile don't have a big importance.\n",
    "- The third one is comparable (even if lower) with the mean of the values, and that means that it's important in the process of classification.\n",
    "\n",
    "Given this results, we can state that this behaviour is reasonable since giving importances to different features instead of giving importance to a small subset can prevent overfitting in a decision tree. Furthermore, both the bar plots above, (the distribution of importance and the importance of each feature) show that the majority of the features (approximately 75%) don't have a big importance. This plot matches our previous assumption given the description of the feature importances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
