{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before starting\n",
    "## Open in notebook viewer to see the [online version](http://nbviewer.jupyter.org/github/puccife/ADAHomeworks/blob/master/homework03/homework03.ipynb)\n",
    "\n",
    "Be sure that you have already installed the following libraries. In particular:\n",
    "- **geopy**, used to obtain location of a geographical address\n",
    "- **branca**, used to create a custom figure with multiple sub-figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import folium\n",
    "import re\n",
    "from numpy import interp\n",
    "from geopy.geocoders import Nominatim\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium.plugins.measure_control import MeasureControl\n",
    "from folium import plugins\n",
    "import branca\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common function for all tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding cloropleth to map\n",
    "Function used to add a  cloropleth map based on:\n",
    "- dataframe that contains the values that we want to represent on the cloroplet\n",
    "- map, where to add the cloropleth map\n",
    "- key, used to match cloropleth map ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCloropleth(dataframe, map_, feature, legend_name, key, scale_color,json_data,object_):\n",
    "    map_.choropleth(geo_data=json_data,\n",
    "             data=dataframe,\n",
    "             columns=[key, feature],\n",
    "             key_on='feature.id',\n",
    "             fill_color=scale_color, \n",
    "             fill_opacity=0.5, \n",
    "             line_opacity=0.2,\n",
    "             highlight=True,\n",
    "             legend_name=legend_name, topojson = object_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and topojson files\n",
    "- **europe.topojson.json** - Contains the european countries abbreviations and arcs\n",
    "- **tepsr_wc170.tsv** - Dataset we are using for european unemployment rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "europe_topo_path = './topojson/europe.topojson.json'\n",
    "path = './datasets/tepsr_wc170.tsv'\n",
    "object_europe = 'objects.europe'\n",
    "geolocator = Nominatim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "We choose this dataset in particular because it had unemployment rate values for Switzerland.\n",
    "\n",
    "\n",
    "The dataset describes the unemployment rate based on the **EU Labour force** survey, where the unemployment rate represents unemployed persons as a percentage of the labour force and the labour force is the total number of people employed and unemployed.\n",
    "The dataset is divided into several **age categories**:\n",
    "\n",
    "- 15-24 Years old. \n",
    "- 15-74 Years old, which represents all the age margins.\n",
    "- 20-64 Years old\n",
    "- 25-29 Years old\n",
    "- 25-54 Years old\n",
    "- 55-64 Years old\n",
    "\n",
    "Furthermore, we have unemployment rate starting from 2005 upto 2016.\n",
    "\n",
    "The countries available in the dataset are :\n",
    "\n",
    "- The 28 European Union countries.\n",
    "- 3 of the European Free Trade Association (EFTA) [ **Liechtenstein** is not avialable ].\n",
    "- 2 of the EU candidate countries, in particular [ **Turkey**, **The former Yugoslav Republic of Macedonia** ].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading json files and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_json_data = json.load(open(europe_topo_path))\n",
    "df = pd.read_csv(path,sep=',|\\t',engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used for Task 1\n",
    "### Changing values from string to float\n",
    "\n",
    "In the dataset the columns that contain the unemployment rates are strings and some of them have letters included.\n",
    "This function takes the strings in the unemployment rate column and extracts the float number and returns a list of float numbers containting the unemployment rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_float(unemployment_pc_act):\n",
    "    float_list = []\n",
    "    # Extracting all the unemployment rates with type string into a list\n",
    "    string_list = list(unemployment_pc_act['Unemployment Rate'])\n",
    "    # Splitting the number from letter in the string, for example from '4.6 b' to '4.6' .\n",
    "    for value in string_list:\n",
    "        float_list.extend((re.findall(\"\\d+\\.\\d+\", value)))\n",
    "    float_list = [float(i) for i in float_list]\n",
    "    return float_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the dataframe\n",
    "\n",
    "Through this function we will be able to pick a certain year and a certain age category to be used in the map.\n",
    "The output would be a dataframe that contains the abbreviation of the country as index, Age range and Unemployment Rate.\n",
    "\n",
    "Some refinements are made regarding the abbreviations of 2 specific countries [ **United Kingdom**, and **Greece** ] from [ **EL , UK** to **GR , GB** ]  to match the abbreviations found in the topojson file.\n",
    "\n",
    "The unemployment rate column is changed to float using the function described above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataframe_1(df,year,age):\n",
    "    \n",
    "    # Setting index to the dataframe and using only the columns set to the criteria passed in the function.\n",
    "    df = df.set_index('geo\\\\time')\n",
    "    extracted_columns = str('age') + '|' + str(year)\n",
    "    unemployment_pc = df.filter(regex = (extracted_columns))\n",
    "    unemployment_pc.index.names=['Abreviations']\n",
    "    unemployment_pc.columns=['Age Range','Unemployment Rate']\n",
    "    unemployment_pc_act = unemployment_pc[unemployment_pc['Age Range']== age]\n",
    "    \n",
    "    # Changing UK and EL to GR and GB to match the topojson file.\n",
    "    indexes = unemployment_pc_act.index.tolist()\n",
    "    idx_1 = indexes.index('EL')\n",
    "    idx_2 = indexes.index('UK')\n",
    "    indexes[idx_1] = 'GR'\n",
    "    indexes[idx_2] = 'GB'\n",
    "    unemployment_pc_act.index = indexes\n",
    "    unemployment_pc_act.index.name = 'Abreviations'\n",
    "    \n",
    "    # Cleaning the unemployment rate column and changing it to float data type column.\n",
    "    unemployment_rate = string_to_float(unemployment_pc_act)\n",
    "    unemployment_pc_act = unemployment_pc_act.drop('Unemployment Rate',axis=1)\n",
    "    unemployment_pc_act['Unemployment Rate'] = unemployment_rate\n",
    "    \n",
    "    return unemployment_pc_act\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting european countries and their abreviations from topojson\n",
    "The europe.topojson file contains 51 countries with their abbreviations. This function extracts the name of the country with the corresponding abbreviation given to it inside the topojson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def europe_abreviations(topo_json_data):\n",
    "    \n",
    "    europe_countries = pd.DataFrame()\n",
    "    # Extracting country names from the topojson file and adding it to the dataframe.\n",
    "    europe_countries['Country'] = [country['properties']['NAME'] for country in topo_json_data['objects']['europe']['geometries']]\n",
    "    # Extracting country abbreviations from the topojson file and adding it to the dataframe.\n",
    "    europe_countries['Abreviations'] = [abrv['id'] for abrv in topo_json_data['objects']['europe']['geometries']]\n",
    "    # Setting dataframe index to the abbreviations of the countries.\n",
    "    europe_countries = europe_countries.set_index('Abreviations')\n",
    "    return europe_countries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating HTML popup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHtml(location_name, rate_value, label):\n",
    "    return \"\"\"\n",
    "    <style>\n",
    "    h3 {\n",
    "        color: blue;\n",
    "        text-align: center;\n",
    "        border-bottom: 1px solid rgb(200, 200, 200);\n",
    "    }\n",
    "    h5 {\n",
    "        text-align: left;\n",
    "        font-family: verdana;\n",
    "        font-size: 12px;\n",
    "        border-bottom: 1px solid rgb(200, 200, 200);\n",
    "    }\n",
    "    </style>\n",
    "    <h3> \"\"\" + location_name + \"\"\"</h3><br>\n",
    "    <h4>\n",
    "        \"\"\"+ label +\"\"\": <em>\"\"\"+ rate_value +\"\"\" </em>\n",
    "    </h4>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataframe to be used in the folium map\n",
    "In here we merge both the dataframe that includes all of our unemployment rate data and the one that includes the european countries with their abbreviations.\n",
    "\n",
    "The output would contain only the countries that are listed in both dataframes, excluding the unnecessary data.\n",
    "We also return a list of the abbreviations of the countries included in that dataframe to be used to clean the topojson file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abreviations_rate(dataframe_1,dataframe_2):\n",
    "    \n",
    "    # Joining the dataset dataframe and the country/abbreviations.\n",
    "    country_rate = dataframe_1.join(dataframe_2)\n",
    "    # Remove the countries that aren't present in the dataset.\n",
    "    country_rate = country_rate[country_rate['Country'].notnull()]\n",
    "    #abrv_rate = country_rate['Unemployment Rate'].to_frame()\n",
    "    abreviations = list(country_rate.index.values)\n",
    "    # Returning the dataframe that is going to be used in the map.\n",
    "    country_rate = country_rate.reset_index()\n",
    "    country_rate = country_rate.drop('Age Range',axis=1)\n",
    "    return abreviations,country_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cleaning topojson to contain only european countries\n",
    "As mentioned before the europe.topojson file contains 51 countries.\n",
    "Since we need only to have the european countries listed in our dataset for our map, we remove the countries that are not listed in the output of the dataframe created above using the abbreviations we extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_topojson(topo_json_data,abreviations):\n",
    "    new_list = []\n",
    "    # Extracting the country data we need that are only present in our dataset, using the list of country\n",
    "    # abbreviations we got above.\n",
    "    topo_data = topo_json_data\n",
    "    new_list [:] = [ country for country in topo_data['objects']['europe']['geometries'] if country['id'] in abreviations]\n",
    "    topo_data['objects']['europe']['geometries'] = new_list\n",
    "    return topo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Markers to map\n",
    "Function used to add Circular markers to the map.\n",
    "- for each country we obtain its latitude and longitude.\n",
    "- at this latitude and longitude we plot a circleMarker.\n",
    "- this marker has a html popup displaying the informations related to the country.\n",
    "- the size of the marker is proportional to the rate of the information we are representing. In other words, higher value of unemployement rate = bigger circle.\n",
    "- we add the markers to a marker cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addMarkersToMap_(dataframe, map_, key, feature, marker_color, legend_name):\n",
    "    marker_cluster = MarkerCluster().add_to(map_)\n",
    "    c = MeasureControl()\n",
    "    c.add_to(map_)\n",
    "    for index,row in dataframe.iterrows():\n",
    "        place = dataframe.at[index,key] + \", \" + dataframe.at[index,'Abreviations']\n",
    "        location = geolocator.geocode(place)\n",
    "        dataframe.loc[index, 'Latitude'] = location.latitude\n",
    "        dataframe.loc[index, 'Longitude'] = location.longitude\n",
    "        html = getHtml(place, str(dataframe.at[index,feature]), legend_name)\n",
    "        folium.CircleMarker(\n",
    "            location=[location.latitude, location.longitude],\n",
    "            radius=dataframe.at[index,feature],\n",
    "            fill=True,\n",
    "            fill_color = marker_color,\n",
    "            color = marker_color,\n",
    "            popup=folium.Popup(html),\n",
    "        ).add_to(marker_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folium Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "europe_map_2016 = folium.Map(location=[51,10], zoom_start=4)\n",
    "europe_map_2006 = folium.Map(location=[51,10], zoom_start=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the functions listed above to get the detailed map.\n",
    "\n",
    "You can manipulate the variables year and age_category to get different maps.\n",
    "\n",
    "### Obtaining data for 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2016\n",
    "age_category = 'Y15-74'\n",
    "\n",
    "# Clean dataframe w.r.t. age category and year.\n",
    "unemployment_pc_act = cleanDataframe_1(df,year,age_category)\n",
    "# European country dataframe with abbreviations.\n",
    "europe_abrv= europe_abreviations(topo_json_data)\n",
    "# European countries abbreviations and their unemployment rate\n",
    "abreviations, country_rate = abreviations_rate(unemployment_pc_act,europe_abrv)\n",
    "# Topojson file containing only the countries present in our dataset\n",
    "topo_json_data = clean_topojson(topo_json_data,abreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "addCloropleth(country_rate,europe_map_2016,'Unemployment Rate','Percentage of Unemployment(%) 2016','Abreviations','YlOrRd',topo_json_data,object_europe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "GeocoderUnavailable",
     "evalue": "Service not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1318\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m         self.sock = self._create_connection(\n\u001b[0;32m--> 936\u001b[0;31m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 101] Network is unreachable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/geopy/geocoders/base.py\u001b[0m in \u001b[0;36m_call_geocoder\u001b[0;34m(self, url, timeout, raw, requester, deserializer, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# pylint: disable=W0703\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 544\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1361\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 101] Network is unreachable>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mGeocoderUnavailable\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-951799d11a37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maddMarkersToMap_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meurope_map_2016\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Country'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unemployment Rate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Percentage of Unemployment (%) 2016'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-e68e359cae5d>\u001b[0m in \u001b[0;36maddMarkersToMap_\u001b[0;34m(dataframe, map_, key, feature, marker_color, legend_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Abreviations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeolocator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeocode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Latitude'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatitude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Longitude'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlongitude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/geopy/geocoders/osm.py\u001b[0m in \u001b[0;36mgeocode\u001b[0;34m(self, query, exactly_one, timeout, addressdetails, language, geometry)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.geocode: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         return self._parse_json(\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_geocoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexactly_one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/geopy/geocoders/base.py\u001b[0m in \u001b[0;36m_call_geocoder\u001b[0;34m(self, url, timeout, raw, requester, deserializer, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mGeocoderTimedOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Service timed out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;34m\"unreachable\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mGeocoderUnavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Service not available'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mGeocoderTimedOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Service timed out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGeocoderUnavailable\u001b[0m: Service not available"
     ]
    }
   ],
   "source": [
    "addMarkersToMap_(country_rate, europe_map_2016, 'Country', 'Unemployment Rate', 'blue', 'Percentage of Unemployment (%) 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining data for 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2006\n",
    "age_category = 'Y15-74'\n",
    "\n",
    "# Clean dataframe w.r.t. age category and year.\n",
    "unemployment_pc_act = cleanDataframe_1(df,year,age_category)\n",
    "# European country dataframe with abbreviations.\n",
    "europe_abrv= europe_abreviations(topo_json_data)\n",
    "# European countries abbreviations and their unemployment rate\n",
    "abreviations, country_rate = abreviations_rate(unemployment_pc_act,europe_abrv)\n",
    "# Topojson file containing only the countries present in our dataset\n",
    "topo_json_data = clean_topojson(topo_json_data,abreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addCloropleth(country_rate,europe_map_2006,'Unemployment Rate','Percentage of Unemployment(%) 2006','Abreviations','YlOrRd',topo_json_data,object_europe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addMarkersToMap_(country_rate, europe_map_2006, 'Country', 'Unemployment Rate', 'blue', 'Percentage of Unemployment (%) 2006')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying \n",
    "Storing the maps in multiple html figures, using **branca**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = branca.element.Figure()\n",
    "d1 = f.add_subplot(1, 2, 1)\n",
    "d2 = f.add_subplot(1, 2, 2)\n",
    "d1.add_child(europe_map_2006)\n",
    "d2.add_child(europe_map_2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Above we have 2 maps, the unemployment rates from 2006 and 2016.\n",
    "On a larger scale, we can see that most of the countries had a lower unemployment rate in 2006 than 2016.\n",
    "\n",
    "Iceland is the country with the least rate in 2006 and 2016 with 2.8% and 3.0% respectively.\n",
    "\n",
    "Macedonia shows a great improvement from 36.1% to 23.7%.\n",
    "\n",
    "In 2006, Switzerland had 4% unemployment rate, ranking 5th between all the displayed countries where Iceland, Norway, Netherlands, and Denmark topped the ranks all fluctuating in the 1% margin.\n",
    "\n",
    "In 2016, Switzerland had 5% unemployment rate, ranking 7th between all the displayed countries where Iceland, Czech Republic, Germany, Malta, Norway, and the United Kingdom topped the ranks all fluctuating in the 2% margin.\n",
    "\n",
    "A lot of factors effect the unemployment rate such as the Economical growth of a country or the technological sector advancements.\n",
    "\n",
    "We can clearly notice that Greece went from 9.0% unemployment rate to 23.6% in a decade due to the financial crisis of 2007."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of functions for task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map between cantons and sign\n",
    "Dictionary used to map every Country to its Identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons = {\n",
    "        'Zurigo': 'ZH',\n",
    "        'Berna': 'BE',\n",
    "        'Lucerna': 'LU',\n",
    "        'Uri': 'UR',\n",
    "        'Svitto': 'SZ',\n",
    "        'Obwaldo': 'OW',\n",
    "        'Nidwaldo': 'NW',\n",
    "        'Glarona': 'GL',\n",
    "        'Zugo': 'ZG',\n",
    "        'Friburgo': 'FR',\n",
    "        'Soletta': 'SO',\n",
    "        'Basilea-Città': 'BS',\n",
    "        'Basilea-Campagna': 'BL',\n",
    "        'Sciaffusa': 'SH',\n",
    "        'Appenzello Esterno': 'AR',\n",
    "        'Appenzello Interno': 'AI',\n",
    "        'San Gallo': 'SG',\n",
    "        'Grigioni': 'GR',\n",
    "        'Argovia': 'AG',\n",
    "        'Turgovia': 'TG',\n",
    "        'Ticino': 'TI',\n",
    "        'Vaud': 'VD',\n",
    "        'Vallese': 'VS',\n",
    "        'Neuchâtel': 'NE',\n",
    "        'Ginevra': 'GE',\n",
    "        'Giura': 'JU'\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataframe\n",
    "- For each csv file we keep only the meaningful columns. \n",
    "- We create a new column which represents the unemployement rate considering people that already have a job but are looking for another one.\n",
    "- For each canton we add also the column with the relative identificator or \"Sign\" according to the dictionary \"cantons\" specified above.\n",
    "- We convert every value in float, rounded to one decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataframe(dataframe, feature):\n",
    "    dataframe = dataframe.filter(regex=(feature + \".*|Cantone\"))\n",
    "    dataframe = dataframe.apply(lambda x: x.str.replace(\"'\",''))\n",
    "    new_header = dataframe.iloc[0] #grab the first row for the header\n",
    "    dataframe = dataframe[1:] #take the data less the header row\n",
    "    dataframe.columns = new_header #set the header row as the df header\n",
    "    dataframe.columns.values[0] = 'Cantone'\n",
    "    dataframe = dataframe.set_index('Cantone')\n",
    "    selected_columns = ['Tasso di disoccupazione', 'Disoccupati registrati', 'Persone in cerca dimpiego']\n",
    "    dataframe = dataframe[selected_columns]\n",
    "    cols=[i for i in dataframe.columns if i not in 'Cantone']\n",
    "    for col in cols:\n",
    "        dataframe[col]=pd.to_numeric(dataframe[col])\n",
    "    dataframe = dataframe.drop('Totale')\n",
    "    for index,row in dataframe.iterrows():\n",
    "        if feature == 'Totale':\n",
    "            dataframe.loc[index, 'Disoccupati registrati'] = dataframe.at[index, 'Disoccupati registrati']/9\n",
    "            dataframe.loc[index, 'Persone in cerca dimpiego'] = dataframe.at[index, 'Persone in cerca dimpiego']/9\n",
    "        dataframe.loc[index, 'Looking for a job rate'] = dataframe.at[index, 'Persone in cerca dimpiego'] * dataframe.at[index, 'Tasso di disoccupazione'] / dataframe.at[index, 'Disoccupati registrati']\n",
    "        dataframe.loc[index, 'Sign'] = cantons[index]  \n",
    "    dataframe = dataframe.round(1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding cloropleth to map\n",
    "Function used to add a  cloropleth map based on:\n",
    "- dataframe that contains the values that we want to represent on the cloroplet\n",
    "- map, where to add the cloropleth map\n",
    "- key, used to match cloropleth map ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCloropleth(dataframe, map_, feature, legend_name, key, scale_color,json_data,object_):\n",
    "    map_.choropleth(geo_data=json_data,\n",
    "             data=dataframe,\n",
    "             columns=[key, feature],\n",
    "             key_on='feature.id',\n",
    "             fill_color=scale_color, \n",
    "             fill_opacity=0.5, \n",
    "             line_opacity=0.2,\n",
    "             highlight=True,\n",
    "             legend_name=legend_name, topojson = object_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Markers to map\n",
    "Function used to add Circular markers to the map.\n",
    "- for each canton we obtain its latitude and longitude\n",
    "- at this latitude and longitude we plot a circleMarker\n",
    "- this marker has a html popup displaying the informations related to the single canton\n",
    "- the size of the marker is proportional to the rate of the information we are representing. In other words, higher value of unemployement rate = bigger circle.\n",
    "- we add the markers to a marker cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addMarkersToMap(dataframe, map_, key, feature, marker_color, legend_name, second_label):\n",
    "    marker_cluster = MarkerCluster().add_to(map_)\n",
    "    c = MeasureControl()\n",
    "    c.add_to(map_)\n",
    "    for index,row in dataframe.iterrows():\n",
    "        place = dataframe.at[index,key]\n",
    "        if (place == 'AI'): # to avoid errors in the creation of the location\n",
    "            place = 'Appenzell Innerrhoden'\n",
    "        elif (place == 'SH'): # to avoid errors in the creation of the location\n",
    "            place = 'Schaffhausen'\n",
    "        location = geolocator.geocode(place + ', Schweiz/Suisse/Svizzera/Svizra') # to get coordinates\n",
    "        dataframe.loc[index, 'Latitude'] = location.latitude\n",
    "        dataframe.loc[index, 'Longitude'] = location.longitude\n",
    "        html = getHtml(location.address.split(',', 1)[0], \n",
    "                       str(dataframe.at[index,feature]), \n",
    "                       legend_name, second_label, \n",
    "                       str(dataframe.at[index, second_label]))\n",
    "        folium.CircleMarker(\n",
    "            location=[location.latitude, location.longitude],\n",
    "            radius=dataframe.at[index,feature]*5,\n",
    "            fill=True,\n",
    "            fill_color = marker_color,\n",
    "            color = marker_color,\n",
    "            popup=folium.Popup(html),\n",
    "        ).add_to(marker_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating HTML Popup\n",
    "Html popup used to display the data of a canton in a pretty way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHtml(location_name, rate_value, label, second_label, value_second_label):\n",
    "    if second_label == 'Persone in cerca dimpiego':\n",
    "        second_label = 'People are looking for a job'\n",
    "    elif second_label == 'Disoccupati registrati':\n",
    "        second_label = 'Unemployed registered'\n",
    "    return \"\"\"\n",
    "    <style>\n",
    "    h3 {\n",
    "        color: blue;\n",
    "        text-align: center;\n",
    "        border-bottom: 1px solid rgb(200, 200, 200);\n",
    "    }\n",
    "    h5 {\n",
    "        text-align: left;\n",
    "        font-family: verdana;\n",
    "        font-size: 12px;\n",
    "        border-bottom: 1px solid rgb(200, 200, 200);\n",
    "    }\n",
    "    </style>\n",
    "    <div>\n",
    "    <h3> \"\"\" + location_name + \"\"\"</h3><br>\n",
    "    <h5>\n",
    "        \"\"\"+ label +\"\"\": <em>\"\"\"+ rate_value +\"\"\" </em>\n",
    "    </h5>\n",
    "    <h5>\n",
    "        \"\"\"+ second_label +\"\"\": \"\"\"+ value_second_label +\"\"\" \n",
    "    </h5>\n",
    "    </div>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 \n",
    "\n",
    "##  Unemployement rate of Switzerland, by cantons\n",
    "We will solve task 2 using the functions specified above. Definition and usage of each function are specified above.\n",
    "\n",
    "We use the data retrieved from the [amstat](https://www.amstat.ch/) website. We choose our csv dataset with the following features:\n",
    "- rate of unemployed registered\n",
    "- unemployement rate\n",
    "- number of people searching for a job (unemployed or employed)\n",
    "\n",
    "With this data we will derive the feature:\n",
    "- unemployement rate of people without a job\n",
    "\n",
    "We plot the two rates in different maps.\n",
    "\n",
    "### Assumptions\n",
    "1. We choose to repeat the same analysis on three different situations. In particular we select:\n",
    "    - unemployement values of August 2017.\n",
    "    - unemployement values of January 2017\n",
    "    - avg unemployement values from January to September 2017.\n",
    "    \n",
    "  In this way we can compare the value of unemployement rate in different seasons of the year with the average data of the current year to analyze some possible **seasonal bias**, since the **results may depend on the time of the year that we are evaluating**.\n",
    "  \n",
    "2. Given the definition from the website, the **unemployed registered** are:\n",
    "    >'Persone registrate presso gli uffici regionali di collocamento, senza un impiego e immediatamente collocabili.    E’ irrilevante sapere se esse percepiscono o meno un’indennità di disoccupazione.\n",
    "   In other words (english), the **unemployed registered** are the persons **without** a job and immediately ready for placement. \n",
    "   \n",
    "3. Given the statement from the [website FAQ](https://www.amstat.ch/v2/faq.jsp?lang=it):\n",
    "    > Il tasso di disoccupazione è calcolato in base ai «disoccupati senza lavoro».\n",
    "\n",
    "    We can assume that the unemployement rate is the rate related to the **number of people unemploed (without a job)**. \n",
    "    For this reason we expect ** rate of people looking for a job** to be higher than the actual rate, since it includes people that are looking for a job. And we can deduce its value with a simple proportion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset paths\n",
    "- **tasso_year.csv**, contains the csv file with the unemployed rate of Switzerland, in Italian language.\n",
    "- **ch-cantons.topojson.json**, contains the topojson file with the map of switzerland cantons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amstat_year_path = 'datasets/tasso_year.csv'\n",
    "cantons_geo_path = r'topojson/ch-cantons.topojson.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geolocator of Geopy used to get the location of the cantons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading csv file\n",
    "Storing csv files into different dataframes, to process them in different ways. After cleaning the dataframes:\n",
    "1. **df**, contains the values related only to the month of August 2017\n",
    "1. **df_jan**, contains the values related only to the month of January 2017\n",
    "2. **df_year**, contains all the values from Jan to Sept 2017, last column Total contains the avg of the previous selected months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(amstat_year_path, encoding='utf_16')\n",
    "df_year = pd.read_csv(amstat_year_path, encoding='utf_16')\n",
    "df_jan = pd.read_csv(amstat_year_path, encoding='utf_16')\n",
    "cantons_json_data = json.load(open(cantons_geo_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year = cleanDataframe(df_year, 'Totale')\n",
    "df = cleanDataframe(df, 'Agosto')\n",
    "df_jan = cleanDataframe(df_jan, 'Gennaio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Folium Map\n",
    "Creating six different folium maps used to represent:\n",
    "- Unemployement rate of August 2017 (considering only unemployed people)\n",
    "- Avg rate of people looking for a job in August 2017\n",
    "\n",
    "- Avg Unemployement rate from Jan to Sept 2017 (considering only unemployed people)\n",
    "- Avg rate of people looking for a job from Jan to Sept 2017\n",
    "\n",
    "- Unemployement rate of January 2017 (considering only unemployed people)\n",
    "- Avg rate of people looking for a job in January 2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map([46.8,8], \n",
    "               tiles='cartodbpositron', \n",
    "               zoom_start=7)\n",
    "m2 = folium.Map([46.8,8], \n",
    "               tiles='cartodbpositron', \n",
    "               zoom_start=7)\n",
    "m3 = folium.Map([46.8,8], \n",
    "               tiles='cartodbpositron', \n",
    "               zoom_start=7)\n",
    "m4 = folium.Map([46.8,8], \n",
    "               tiles='cartodbpositron', \n",
    "               zoom_start=7)\n",
    "m5 = folium.Map([46.8,8], \n",
    "               tiles='cartodbpositron', \n",
    "               zoom_start=7)\n",
    "m6 = folium.Map([46.8,8], \n",
    "               tiles='cartodbpositron', \n",
    "               zoom_start=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding cloropleth map\n",
    "Adding the cloropleth related to each map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addCloropleth(df, m, 'Tasso di disoccupazione','Unemployement rate', 'Sign','YlGnBu', cantons_json_data,'objects.cantons')\n",
    "addCloropleth(df, m2, 'Looking for a job rate','Looking for a job rate', 'Sign','YlOrRd', cantons_json_data,'objects.cantons')\n",
    "addCloropleth(df_year, m3, 'Tasso di disoccupazione','Unemployement rate', 'Sign','YlGnBu', cantons_json_data,'objects.cantons')\n",
    "addCloropleth(df_year, m4, 'Looking for a job rate','Looking for a job rate', 'Sign','YlOrRd', cantons_json_data,'objects.cantons')\n",
    "addCloropleth(df_jan, m5, 'Tasso di disoccupazione','Unemployement rate', 'Sign','YlGnBu', cantons_json_data,'objects.cantons')\n",
    "addCloropleth(df_jan, m6, 'Looking for a job rate','Looking for a job rate', 'Sign','YlOrRd', cantons_json_data,'objects.cantons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding marker with custom popup for each canton\n",
    "Adding the markers related to each map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addMarkersToMap(df, m2, 'Sign', 'Looking for a job rate','red','Looking for a job rate','Persone in cerca dimpiego')\n",
    "addMarkersToMap(df, m, 'Sign', 'Tasso di disoccupazione','blue','Unemployement rate', 'Disoccupati registrati')\n",
    "addMarkersToMap(df_year, m4, 'Sign', 'Looking for a job rate','red','Looking for a job rate','Persone in cerca dimpiego')\n",
    "addMarkersToMap(df_year, m3, 'Sign', 'Tasso di disoccupazione','blue','Unemployement rate','Disoccupati registrati')\n",
    "addMarkersToMap(df_jan, m6, 'Sign', 'Looking for a job rate','red','Looking for a job rate','Persone in cerca dimpiego')\n",
    "addMarkersToMap(df_jan, m5, 'Sign', 'Tasso di disoccupazione','blue','Unemployement rate', 'Disoccupati registrati')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying 🖥\n",
    "Storing the maps in multiple html figures, using **branca**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = branca.element.Figure()\n",
    "d1 = f.add_subplot(1, 2, 1)\n",
    "d2 = f.add_subplot(1, 2, 2)\n",
    "d1.add_child(m)\n",
    "d2.add_child(m2)\n",
    "f_year = branca.element.Figure()\n",
    "d1_year = f_year.add_subplot(1, 2, 1)\n",
    "d2_year = f_year.add_subplot(1, 2, 2)\n",
    "d1_year.add_child(m3)\n",
    "d2_year.add_child(m4)\n",
    "f_jan = branca.element.Figure()\n",
    "d1_jan = f_jan.add_subplot(1, 2, 1)\n",
    "d2_jan = f_jan.add_subplot(1, 2, 2)\n",
    "d1_jan.add_child(m5)\n",
    "d2_jan.add_child(m6)\n",
    "print(\"Branca figures created...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results as html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.save(os.path.join('results', 'unemployement_aug.html'))\n",
    "f_year.save(os.path.join('results', 'unemployement_year.html'))\n",
    "f_jan.save(os.path.join('results', 'unemployement_jan.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Observations 🔍\n",
    "\n",
    "Given the results above we observe that we can not pick the unemployement rate from a month randomly due to the fact that the unemployement rate is **strongly influenced by the season of the year** that we are considering.\n",
    "Given the avg unemployement rate of the 2017 we can see how the unemployement rate of January is evidently higher than the unemployement rate of August. \n",
    "\n",
    "For instance in canton **Valais/Wallis** it clearly changes:\n",
    "\n",
    "|            | Unemployement rate | Looking for a job rate |\n",
    "|:----------:|:------------------:|:----------------------:|\n",
    "|   January  |         5.2        |           7.5          |\n",
    "|   August   |         2.9        |           4.7          |\n",
    "| Yearly avg |         3.6        |           5.6          |\n",
    "\n",
    "We can not really give an accurate explanation for this trend, since these values could be influenced by many factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing results in a IFrame ⚠️\n",
    "- On the left is shown the map for the unemployement rate in the selected period (considering only jobless people).\n",
    "- On the right is shown the map for the rate of people looking for a job (considering both employed and unemployed people)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### January"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IFrame(src=\"results/unemployement_jan.html\",width=1000,height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src=\"results/unemployement_aug.html\",width=1000,height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Yearly average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src=\"results/unemployement_year.html\",width=1000,height=800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
